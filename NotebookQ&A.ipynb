{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cef31e-cbe2-4954-a7bd-832a70831d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0991cfcc-e36c-41ed-885b-9899a270fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19f55f6-008c-4605-a43c-f5d0d3da9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa589793-f21b-4af0-9ad5-7eadc604686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from Ng_MachineLearningYearning.pdf...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extracting text from the PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \n",
    "    text = \"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Specify PDF file path\n",
    "pdf_file_path = \"Ng_MachineLearningYearning.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "print(f\"Extracted text from {pdf_file_path[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1c291b-1d4c-4143-9d0a-34a9f6aa40a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of extracted text: 161121\n",
      "Preview of extracted text:  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Machine Learning Yearning is a\n",
      " \n",
      " \n",
      "deeplearning.ai project.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "© 2018 Andrew Ng. All Rights Reserved.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Page 2\n",
      "Machine Learning Yearning-Draft\n",
      "Andrew Ng\n",
      "  \n",
      "Table of Contents\n",
      " \n",
      " \n",
      "1 Why Machine Learning Strategy\n",
      " \n",
      "2 How to use this book to help your team\n",
      " \n",
      "3 Prerequisites and Notation\n",
      " \n",
      "4 Scale drives machine learning progress\n",
      " \n",
      "5 Your development and test sets\n",
      " \n",
      "6 Your dev and test sets should come from the same distribution\n",
      " \n",
      "7 How large do the dev/test set\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of extracted text: {len(pdf_text)}\")\n",
    "if len(pdf_text) > 0:\n",
    "    print(f\"Preview of extracted text: {pdf_text[:500]}\")  # Preview the first 500 characters\n",
    "else:\n",
    "    print(\"No text was extracted. The PDF may not contain extractable text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a56b9-032b-4516-9d52-00f7442f16a7",
   "metadata": {},
   "source": [
    "The text extraction step is complete. \n",
    "However, the preview reveals that there may be unnecessary whitespace or extra line breaks in the text.\n",
    "This can be cleaned up during the preprocessing step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef21676-a912-40bd-879c-4f515753c988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cleaned text: 153889\n",
      "Preview of cleaned text: Machine Learning Yearning is a deeplearning.ai project. © 2018 Andrew Ng. All Rights Reserved. Page 2 Machine Learning Yearning-Draft Andrew Ng Table of Contents 1 Why Machine Learning Strategy 2 How to use this book to help your team 3 Prerequisites and Notation 4 Scale drives machine learning progress 5 Your development and test sets 6 Your dev and test sets should come from the same distribution 7 How large do the dev/test sets need to be? 8 Establish a single-number evaluation metric for you\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Removing multiple spaces, tabs, and newlines\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Preprocess the extracted text\n",
    "cleaned_pdf_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Check the cleaned text\n",
    "print(f\"Length of cleaned text: {len(cleaned_pdf_text)}\")\n",
    "print(f\"Preview of cleaned text: {cleaned_pdf_text[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49724e9-cfac-433e-a93d-b87da383add0",
   "metadata": {},
   "source": [
    "The text is now more readable and cleaned from unnecessary line breaks, extra spaces, and special characters.\n",
    "The content seems to be structured, starting with the book's introduction and table of contents, which is perfect for chunking and creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea77d8f7-0b6e-4080-acc1-ff67ef0c4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Chunk the Text into Manageable Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28ad0f0-9cf7-4fb2-8913-dbcc2d1762c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 308\n",
      "First chunk preview: Machine Learning Yearning is a deeplearning.ai project. © 2018 Andrew Ng. All Rights Reserved. Page 2 Machine Learning Yearning-Draft Andrew Ng Table of Contents 1 Why Machine Learning Strategy 2 How to use this book to help your team 3 Prerequisites and Notation 4 Scale drives machine learning progress 5 Your development and test sets 6 Your dev and test sets should come from the same distribution 7 How large do the dev/test sets need to be? 8 Establish a single-number evaluation metric for you\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500):\n",
    "    # Split text into chunks of the specified size\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Split the cleaned text into chunks\n",
    "chunk_size = 500  \n",
    "text_chunks = chunk_text(cleaned_pdf_text, chunk_size)\n",
    "\n",
    "# Display the number of chunks and a preview of the first chunk\n",
    "print(f\"Total chunks created: {len(text_chunks)}\")\n",
    "print(f\"First chunk preview: {text_chunks[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e129ddf-0802-4dac-bae9-fec2df482348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Embeddings for Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9685aa6-6a23-40cb-b5b4-1e58d23cd00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.48.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers faiss-cpu torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c69d6d3-ba5c-4805-abd0-e7e48f5ae3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 308 chunks.\n",
      "Shape of embeddings: (308, 384)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate embeddings for each chunk\n",
    "def generate_embeddings(text_chunks):\n",
    "    embeddings = []\n",
    "    \n",
    "    # Generate embeddings for each chunk of text\n",
    "    for chunk in text_chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use the mean of the token embeddings as the chunk embedding\n",
    "            chunk_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            embeddings.append(chunk_embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "embeddings = generate_embeddings(text_chunks)\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a0677a-ee3b-4606-aa4d-ad33a51b860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build a FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a25b0d3-3d06-4f86-8dea-3c2f448caffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the FAISS library to create an index for storing the embeddings. \n",
    "# FAISS allows us to perform fast similarity searches over the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d05e47-fe37-4cfa-bc1a-e50a7edf6539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in FAISS index: 308\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS index\n",
    "dim = embeddings.shape[1]  # 384 dimensions per embedding\n",
    "index = faiss.IndexFlatL2(dim)  # L2 distance (Euclidean distance)\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Check the number of vectors in the index\n",
    "print(f\"Number of vectors in FAISS index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5f0d54c-af4f-45f4-a496-2fea5b07ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved to faiss_index.pkl\n"
     ]
    }
   ],
   "source": [
    "# Saving FAISS index\n",
    "def save_faiss_index(index, chunks, filename=\"faiss_index.pkl\"):\n",
    "    data = {\n",
    "        \"index\": index,       # The FAISS index\n",
    "        \"chunks\": chunks,     # The text chunks\n",
    "    }\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"FAISS index and metadata saved to {filename}\")\n",
    "\n",
    "save_faiss_index(index, text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696c6ff-1d7e-4851-8443-f527a6d559d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 6: Querying the FAISS Index for Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772182c-be80-49a8-9401-7aebbbda9387",
   "metadata": {},
   "source": [
    "Now we'll proceed with:\n",
    "Converting the question into an embedding.\n",
    "Searching the FAISS index for the most similar chunk.\n",
    "Returning the corresponding chunk as the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26f02f7c-da34-4d5b-86f7-6a27f6fdf6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 similar chunks for the question: 'How to establish a single-number evaluation metric?'\n",
      "\n",
      "Rank 1 (Distance: 17.63):\n",
      "cy is an example of a ​ single-number evaluation metric ​ : You run your classifier on the dev set (or test set), and get back a single number about what fraction of examples it classified correctly. According to this metric, if classifier A obtains 97% accuracy, and classifier B obtains 90% accurac...\n",
      "\n",
      "Rank 2 (Distance: 19.76):\n",
      "valuation metrics makes it harder to compare algorithms. Suppose your algorithms perform as follows: Classifier Precision Recall A 95% 90% B 98% 85% Here, neither classifier is obviously superior, so it doesn’t immediately guide you toward picking one. Classifier Precision Recall F1 score A 95% 90% ...\n",
      "\n",
      "Rank 3 (Distance: 20.95):\n",
      "ne of the most common ways to combine multiple metrics into one. 4 If you want to learn more about the F1 score, see ​ https://en.wikipedia.org/wiki/F1_score ​ . It is the “harmonic mean” between Precision and Recall, and is calculated as 2/((1/Precision)+(1/Recall)). Page 21 Machine Learning Yearni...\n"
     ]
    }
   ],
   "source": [
    "def query_faiss(question, index, top_k=3):\n",
    "    # Step 1: Generate embedding for the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        question_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Step 2: Search the FAISS index for the top-k most similar chunks\n",
    "    D, I = index.search(np.array([question_embedding]), top_k)  # D is distances, I is indices\n",
    "\n",
    "    # Step 3: Retrieve the most similar chunks based on indices\n",
    "    similar_chunks = [text_chunks[i] for i in I[0]]\n",
    "    \n",
    "    return similar_chunks, D[0]  # Return the chunks and their distances\n",
    "\n",
    "# Test the query\n",
    "question = \"How to establish a single-number evaluation metric?\"\n",
    "top_k = 3\n",
    "results, distances = query_faiss(question, index, top_k)\n",
    "\n",
    "print(f\"Top {top_k} similar chunks for the question: '{question}'\")\n",
    "for i, (chunk, distance) in enumerate(zip(results, distances)):\n",
    "    print(f\"\\nRank {i+1} (Distance: {distance:.2f}):\\n{chunk[:300]}...\")  # Preview first 300 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ec7ac-73cb-4fff-9986-1550f19b2843",
   "metadata": {},
   "source": [
    "The question is converted to an embedding using the same model, and FAISS will compare this question embedding to the embeddings of our document chunks.\n",
    "This allows us to retrieve the most relevant chunks that are semantically close to the question, which we can use for further processing or answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b416cb-c97a-4da0-aaf7-1e2c9cedb6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0a469-32ed-4627-a958-339572fe58d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114c457-2bea-42fb-bd87-2c4188de2444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62509e-e1ed-4965-8492-21a413a2b478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d60c12-8958-4997-a921-69997b9b18ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a91cfa-68a9-4209-8ae2-c358496f1e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
